// Copyright (c)  2024  Xiaomi Corporation
import 'dart:async';
import 'dart:typed_data';
import 'dart:collection';
import 'dart:isolate';
import 'dart:math' as math;

import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';
import 'package:path/path.dart' as p;
import 'package:path_provider/path_provider.dart';
import 'package:record/record.dart';

import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
import './utils.dart';

// ğŸ”§ å®Œå…¨å¤åˆ»åç¼–è¯‘APKçš„SenseVoiceé…ç½®
Future<sherpa_onnx.OfflineRecognizer> createSenseVoiceRecognizer() async {
  final modelDir =
      'assets/models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17';

  final senseVoiceConfig = sherpa_onnx.OfflineSenseVoiceModelConfig(
    model: await copyAssetFile('$modelDir/model.int8.onnx'),
    language: '', // ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹è¯­è¨€
    useInverseTextNormalization: true,
  );

  final modelConfig = sherpa_onnx.OfflineModelConfig(
    senseVoice: senseVoiceConfig,
    tokens: await copyAssetFile('$modelDir/tokens.txt'),
    modelType: 'sense_voice',
    numThreads: 2, // ğŸ¯ ä¸¥æ ¼å¤åˆ»ï¼šåç¼–è¯‘APKå¼ºåˆ¶è®¾ç½®ä¸º2çº¿ç¨‹
    debug: false,
  );

  final config = sherpa_onnx.OfflineRecognizerConfig(
    model: modelConfig,
    decodingMethod: 'greedy_search',
    maxActivePaths: 4,
  );

  return sherpa_onnx.OfflineRecognizer(config);
}

// ğŸ”§ å®Œå…¨å¤åˆ»åç¼–è¯‘APKçš„VADé…ç½®
Future<sherpa_onnx.VoiceActivityDetector> createVAD() async {
  final sileroVadConfig = sherpa_onnx.SileroVadModelConfig(
    model: await copyAssetFile('assets/silero_vad.onnx'),
    minSilenceDuration: 0.5, // ğŸ¯ å¤åˆ»åç¼–è¯‘APK: 0.5f
    minSpeechDuration: 0.25, // ğŸ¯ å¤åˆ»åç¼–è¯‘APK: 0.25f
    maxSpeechDuration: 8.0,
    threshold: 0.45,
  );

  final vadConfig = sherpa_onnx.VadModelConfig(
    sileroVad: sileroVadConfig,
    sampleRate: 16000,
    numThreads: 1, // ğŸ¯ ä¸¥æ ¼å¤åˆ»ï¼šåç¼–è¯‘APKçš„VADé»˜è®¤ä½¿ç”¨1çº¿ç¨‹
  );

  return sherpa_onnx.VoiceActivityDetector(
      config: vadConfig, bufferSizeInSeconds: 20);
}

class SimulateStreamingAsrScreen extends StatefulWidget {
  const SimulateStreamingAsrScreen({super.key});

  @override
  State<SimulateStreamingAsrScreen> createState() =>
      _SimulateStreamingAsrScreenState();
}

class _SimulateStreamingAsrScreenState
    extends State<SimulateStreamingAsrScreen> {
  late final TextEditingController _controller;
  late final AudioRecorder _audioRecorder;

  String _title = 'SenseVoiceå¤šè¯­è¨€å®æ—¶è¯†åˆ« (ä¸¥æ ¼å¤åˆ»ç‰ˆ)';
  String _last = '';
  int _index = 0;
  bool _isInitialized = false;
  String _initStatus = 'å‡†å¤‡åˆå§‹åŒ–...';

  sherpa_onnx.OfflineRecognizer? _recognizer;
  sherpa_onnx.VoiceActivityDetector? _vad;
  int _sampleRate = 16000;

  StreamSubscription<RecordState>? _recordSub;
  RecordState _recordState = RecordState.stop;

  // ğŸ¯ éŸ³é¢‘å¤„ç†é€šé“
  StreamController<Float32List> _samplesChannel =
      StreamController<Float32List>.broadcast();

  // ğŸ¯ çŠ¶æ€å˜é‡
  List<double> _buffer = [];
  String _lastText = '';
  int _offset = 0;
  int _windowSize = 6400; // ğŸ¯ å‡å°çª—å£å¤§å° (0.4ç§’)
  bool _isSpeechStarted = false;
  int _startTime = 0;
  bool _added = false;

  // ğŸ¯ æ”¹è¿›çš„æ–‡æœ¬æ˜¾ç¤ºæœºåˆ¶
  List<String> _resultList = [];
  String _currentPartialText = '';
  String _lastStableText = ''; // ğŸ¯ ä¸Šä¸€ä¸ªç¨³å®šçš„æ–‡æœ¬
  int _stableCounter = 0; // ğŸ¯ ç¨³å®šè®¡æ•°å™¨
  Timer? _updateTimer; // ğŸ¯ æ›´æ–°å®šæ—¶å™¨

  // åç¨‹æ§åˆ¶
  StreamSubscription<List<int>>? _audioStreamSub;
  StreamSubscription<Float32List>? _processingStreamSub;

  @override
  void initState() {
    _audioRecorder = AudioRecorder();
    _controller = TextEditingController();

    _recordSub = _audioRecorder.onStateChanged().listen((recordState) {
      _updateRecordState(recordState);
    });

    // ğŸ¯ é¢„åˆå§‹åŒ–ä»¥å‡å°‘å¯åŠ¨æ—¶é—´
    _preInitialize();

    super.initState();
  }

  // ğŸ¯ é¢„åˆå§‹åŒ–æ¨¡å‹
  Future<void> _preInitialize() async {
    setState(() {
      _initStatus = 'æ­£åœ¨åˆå§‹åŒ–SenseVoice...';
    });

    try {
      sherpa_onnx.initBindings();

      setState(() {
        _initStatus = 'æ­£åœ¨åŠ è½½æ¨¡å‹æ–‡ä»¶...';
      });

      _recognizer = await createSenseVoiceRecognizer();

      setState(() {
        _initStatus = 'æ­£åœ¨åˆå§‹åŒ–VAD...';
      });

      _vad = await createVAD();

      setState(() {
        _isInitialized = true;
        _initStatus = 'âœ… åˆå§‹åŒ–å®Œæˆ';
      });

      print('âœ… é¢„åˆå§‹åŒ–å®Œæˆ');
    } catch (e) {
      setState(() {
        _initStatus = 'âŒ åˆå§‹åŒ–å¤±è´¥: $e';
      });
      print('âŒ é¢„åˆå§‹åŒ–å¤±è´¥: $e');
    }
  }

  Future<void> _start() async {
    if (!_isInitialized) {
      setState(() {
        _initStatus = 'è¯·ç­‰å¾…åˆå§‹åŒ–å®Œæˆ...';
      });
      return;
    }

    try {
      if (await _audioRecorder.hasPermission()) {
        const encoder = AudioEncoder.pcm16bits;

        if (!await _isEncoderSupported(encoder)) {
          return;
        }

        const config = RecordConfig(
          encoder: encoder,
          sampleRate: 16000,
          numChannels: 1,
        );

        final stream = await _audioRecorder.startStream(config);

        // ğŸ¯ å¯åŠ¨åŒåç¨‹
        _startAnonymousClass1(stream);
        _startAnonymousClass2();
      }
    } catch (e) {
      print('âŒ å¯åŠ¨å½•éŸ³å¤±è´¥: $e');
    }
  }

  // ğŸ¯ AnonymousClass1: éŸ³é¢‘å½•åˆ¶åç¨‹
  void _startAnonymousClass1(Stream<List<int>> audioStream) {
    print('ğŸ™ï¸ sherpa-onnx-sim-asr: processing samples');

    const int chunkSize = 1600; // 0.1ç§’éŸ³é¢‘å—

    _audioStreamSub = audioStream.listen(
      (rawData) {
        try {
          final bytes = Uint8List.fromList(rawData);
          final shortBuffer = bytes.buffer.asInt16List();

          for (int i = 0; i < shortBuffer.length; i += chunkSize) {
            final end = (i + chunkSize < shortBuffer.length)
                ? i + chunkSize
                : shortBuffer.length;

            final shortChunk = shortBuffer.sublist(i, end);

            final floatChunk = Float32List(shortChunk.length);
            for (int j = 0; j < shortChunk.length; j++) {
              floatChunk[j] = shortChunk[j] / 32768.0;
            }

            if (!_samplesChannel.isClosed) {
              _samplesChannel.add(floatChunk);
            }
          }
        } catch (e) {
          print('AnonymousClass1é”™è¯¯: $e');
        }
      },
      onDone: () {
        print('ğŸ”‡ AnonymousClass1åœæ­¢');
      },
    );
  }

  // ğŸ¯ AnonymousClass2: éŸ³é¢‘å¤„ç†åç¨‹
  void _startAnonymousClass2() {
    print('ğŸ”„ sherpa-onnx-sim-asr: å¯åŠ¨AnonymousClass2');

    // é‡ç½®çŠ¶æ€å˜é‡
    _buffer.clear();
    _lastText = '';
    _lastStableText = '';
    _stableCounter = 0;
    _offset = 0;
    _windowSize = 6400; // 0.4ç§’çª—å£
    _isSpeechStarted = false;
    _startTime = DateTime.now().millisecondsSinceEpoch;
    _added = false;
    _vad?.reset();

    _processingStreamSub = _samplesChannel.stream.listen(
      (floatSamples) {
        _processAnonymousClass2Logic(floatSamples);
      },
      onDone: () {
        print('ğŸ”‡ AnonymousClass2åœæ­¢');
      },
    );
  }

  // ğŸ¯ æ ¸å¿ƒï¼šAnonymousClass2çš„å¤„ç†é€»è¾‘
  void _processAnonymousClass2Logic(Float32List floatSamples) {
    try {
      _buffer.addAll(floatSamples);

      // ğŸ¯ VADå¤„ç† - ä¸»è¦ç”¨äºæ£€æµ‹å®Œæ•´è¯­éŸ³æ®µ
      if (_buffer.length >= 1600) {
        _vad?.acceptWaveform(Float32List.fromList(
            _buffer.sublist(math.max(0, _buffer.length - 1600))));

        // ğŸ¯ å¤„ç†VADæ£€æµ‹åˆ°çš„å®Œæ•´è¯­éŸ³æ®µ
        while (_vad?.isEmpty() == false) {
          final speechSegment = _vad?.front();
          _vad?.pop();

          if (speechSegment != null) {
            _processFinalSpeechSegment(speechSegment);
          }
        }
      }

      // ğŸ¯ æ»‘åŠ¨çª—å£æœºåˆ¶
      if (_buffer.length > _windowSize * 3) {
        final newOffset = _buffer.length - _windowSize;
        _buffer = _buffer.sublist(newOffset);
        _offset = newOffset;
      }

      // ğŸ¯ ä¼˜åŒ–çš„å®æ—¶è¯†åˆ« - å‡å°‘é¢‘ç‡é¿å…è·³åŠ¨
      if (_buffer.length >= _windowSize && _buffer.length % 800 == 0) {
        _performStableRealtimeRecognition();
      }
    } catch (e) {
      print('AnonymousClass2å¤„ç†é”™è¯¯: $e');
    }
  }

  // ğŸ¯ å®Œæ•´è¯­éŸ³æ®µå¤„ç†ï¼ˆé«˜è´¨é‡è¯†åˆ«ï¼‰
  void _processFinalSpeechSegment(dynamic speechSegment) {
    try {
      final stream = _recognizer!.createStream();
      stream.acceptWaveform(
          samples: speechSegment.samples, sampleRate: _sampleRate);

      _recognizer!.decode(stream);
      final result = _recognizer!.getResult(stream);
      final text = result.text.trim();

      if (text.isNotEmpty && text.length > 1) {
        // ğŸ¯ è¿‡æ»¤å¤ªçŸ­çš„ç»“æœ
        _addToResultList(text);
      }

      stream.free();
    } catch (e) {
      print('è¯­éŸ³æ®µå¤„ç†é”™è¯¯: $e');
    }
  }

  // ğŸ¯ ç¨³å®šçš„å®æ—¶è¯†åˆ«ï¼ˆå‡å°‘è·³åŠ¨ï¼‰
  void _performStableRealtimeRecognition() {
    try {
      if (_recognizer == null || _buffer.length < _windowSize) return;

      final windowStart = math.max(0, _buffer.length - _windowSize);
      final windowAudio = Float32List.fromList(_buffer.sublist(windowStart));

      final stream = _recognizer!.createStream();
      stream.acceptWaveform(samples: windowAudio, sampleRate: _sampleRate);

      _recognizer!.decode(stream);
      final result = _recognizer!.getResult(stream);
      final text = result.text.trim();

      // ğŸ¯ ç¨³å®šæ€§æ£€æŸ¥ - å‡å°‘æ–‡å­—è·³åŠ¨
      if (text.isNotEmpty && text.length > 1) {
        if (text == _lastStableText) {
          _stableCounter++;
          if (_stableCounter >= 2) {
            // ğŸ¯ è¿ç»­2æ¬¡ç›¸åŒæ‰æ˜¾ç¤º
            _updateRealtimeTextStable(text);
          }
        } else {
          _lastStableText = text;
          _stableCounter = 1;
        }
      }

      stream.free();
    } catch (e) {
      print('å®æ—¶è¯†åˆ«é”™è¯¯: $e');
    }
  }

  // ğŸ¯ ç¨³å®šçš„å®æ—¶æ–‡æœ¬æ›´æ–°ï¼ˆå‡å°‘è·³åŠ¨ï¼‰
  void _updateRealtimeTextStable(String text) {
    if (text == _lastText) return; // é¿å…é‡å¤æ›´æ–°

    setState(() {
      _currentPartialText = text;
      _lastText = text;

      // ğŸ¯ æ”¹è¿›çš„æ˜¾ç¤ºæ ¼å¼ - å»æ‰"(è¯†åˆ«ä¸­...)"
      String displayText = '';
      for (int i = 0; i < _resultList.length; i++) {
        displayText += '${i + 1}: ${_resultList[i]}\n';
      }

      // ğŸ¯ ç®€æ´çš„å½“å‰è¯†åˆ«æ˜¾ç¤º
      if (_currentPartialText.isNotEmpty) {
        displayText += '${_resultList.length + 1}: $_currentPartialText';
      }

      _controller.value = TextEditingValue(
        text: displayText,
        selection: TextSelection.collapsed(offset: displayText.length),
      );
    });
  }

  // ğŸ¯ æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
  void _addToResultList(String text) {
    setState(() {
      _resultList.add(text);
      _currentPartialText = ''; // æ¸…ç©ºéƒ¨åˆ†ç»“æœ
      _lastText = '';
      _lastStableText = '';
      _stableCounter = 0;

      String displayText = '';
      for (int i = 0; i < _resultList.length; i++) {
        displayText += '${i + 1}: ${_resultList[i]}';
        if (i < _resultList.length - 1) displayText += '\n';
      }

      _controller.value = TextEditingValue(
        text: displayText,
        selection: TextSelection.collapsed(offset: displayText.length),
      );
    });

    print('âœ… æœ€ç»ˆç»“æœ ${_resultList.length}: $text');
  }

  Future<void> _stop() async {
    _audioStreamSub?.cancel();
    _processingStreamSub?.cancel();
    _samplesChannel.close();
    _updateTimer?.cancel();

    // é‡æ–°åˆ›å»ºchannelä¸ºä¸‹æ¬¡ä½¿ç”¨
    _samplesChannel = StreamController<Float32List>.broadcast();

    // é‡ç½®çŠ¶æ€å˜é‡
    _buffer.clear();
    _currentPartialText = '';
    _lastText = '';
    _lastStableText = '';
    _stableCounter = 0;
    _offset = 0;
    _isSpeechStarted = false;
    _added = false;

    await _audioRecorder.stop();
  }

  Future<void> _pause() => _audioRecorder.pause();
  Future<void> _resume() => _audioRecorder.resume();

  void _updateRecordState(RecordState recordState) {
    setState(() => _recordState = recordState);
  }

  Future<bool> _isEncoderSupported(AudioEncoder encoder) async {
    final isSupported = await _audioRecorder.isEncoderSupported(encoder);

    if (!isSupported) {
      debugPrint('${encoder.name} is not supported on this platform.');
      debugPrint('Supported encoders are:');

      for (final e in AudioEncoder.values) {
        if (await _audioRecorder.isEncoderSupported(e)) {
          debugPrint('- ${e.name}');
        }
      }
    }

    return isSupported;
  }

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: Scaffold(
        appBar: AppBar(
          title: Text(_title),
          backgroundColor: Colors.purple[700], // ğŸ¯ ç´«è‰²è¡¨ç¤ºä¸¥æ ¼å¤åˆ»ç‰ˆ
          foregroundColor: Colors.white,
        ),
        body: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: [
            Container(
              padding: const EdgeInsets.all(16),
              margin: const EdgeInsets.all(16),
              decoration: BoxDecoration(
                color: Colors.purple[50], // ğŸ¯ é…è‰²æ›´æ–°
                borderRadius: BorderRadius.circular(8),
                border: Border.all(color: Colors.purple[200]!),
              ),
              child: Column(
                children: [
                  const Icon(Icons.verified, color: Colors.purple), // ğŸ¯ éªŒè¯å›¾æ ‡
                  const SizedBox(height: 8),
                  const Text(
                    'SenseVoiceä¸¥æ ¼å¤åˆ»ç‰ˆ',
                    style: TextStyle(fontWeight: FontWeight.bold),
                  ),
                  const SizedBox(height: 4),
                  const Text(
                    'ğŸ¯ Recognizer: 2çº¿ç¨‹ | ğŸ”§ VAD: 1çº¿ç¨‹ | ğŸ“‹ ä¸¥æ ¼æŒ‰åç¼–è¯‘APK',
                    style: TextStyle(fontSize: 12, color: Colors.grey),
                  ),
                  const SizedBox(height: 4),
                  Text(
                    _initStatus,
                    style: TextStyle(
                      fontSize: 12,
                      color: _isInitialized ? Colors.green : Colors.orange,
                    ),
                  ),
                ],
              ),
            ),
            Expanded(
              child: Container(
                margin: const EdgeInsets.all(16),
                padding: const EdgeInsets.all(16),
                decoration: BoxDecoration(
                  border: Border.all(color: Colors.grey[300]!),
                  borderRadius: BorderRadius.circular(8),
                ),
                child: TextField(
                  maxLines: null,
                  expands: true,
                  controller: _controller,
                  readOnly: true,
                  style: const TextStyle(fontSize: 16),
                  decoration: InputDecoration(
                    border: InputBorder.none,
                    hintText: _isInitialized
                        ? 'å¼€å§‹è¯´è¯ï¼Œä¸¥æ ¼å¤åˆ»åç¼–è¯‘APKçš„å®æ—¶è¯†åˆ«...'
                        : 'æ­£åœ¨åˆå§‹åŒ–ï¼Œè¯·ç¨å€™...',
                  ),
                ),
              ),
            ),
            const SizedBox(height: 20),
            Row(
              mainAxisAlignment: MainAxisAlignment.center,
              children: <Widget>[
                _buildRecordStopControl(),
                const SizedBox(width: 20),
                _buildText(),
              ],
            ),
            const SizedBox(height: 50),
          ],
        ),
      ),
    );
  }

  @override
  void dispose() {
    _audioStreamSub?.cancel();
    _processingStreamSub?.cancel();
    _samplesChannel.close();
    _updateTimer?.cancel();
    _recordSub?.cancel();
    _audioRecorder.dispose();
    _recognizer?.free();
    _vad?.free();
    super.dispose();
  }

  Widget _buildRecordStopControl() {
    late Icon icon;
    late Color color;

    if (_recordState != RecordState.stop) {
      icon = const Icon(Icons.stop, color: Colors.red, size: 30);
      color = Colors.red.withOpacity(0.1);
    } else {
      final theme = Theme.of(context);
      icon = Icon(Icons.mic,
          color: _isInitialized ? theme.primaryColor : Colors.grey, size: 30);
      color = _isInitialized
          ? theme.primaryColor.withOpacity(0.1)
          : Colors.grey.withOpacity(0.1);
    }

    return ClipOval(
      child: Material(
        color: color,
        child: InkWell(
          child: SizedBox(width: 56, height: 56, child: icon),
          onTap: _isInitialized
              ? () => (_recordState != RecordState.stop) ? _stop() : _start()
              : null,
        ),
      ),
    );
  }

  Widget _buildText() {
    if (!_isInitialized) {
      return const Text("åˆå§‹åŒ–ä¸­...");
    } else if (_recordState == RecordState.stop) {
      return const Text("å¼€å§‹å½•éŸ³");
    } else {
      return const Text("åœæ­¢å½•éŸ³");
    }
  }
}
