// Copyright (c)  2024  Xiaomi Corporation
import 'dart:async';
import 'dart:typed_data';
import 'dart:collection';
import 'dart:isolate';
import 'dart:math' as math;

import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';
import 'package:path/path.dart' as p;
import 'package:path_provider/path_provider.dart';
import 'package:record/record.dart';

import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
import './utils.dart';

// ğŸ”§ ä¼˜åŒ–åçš„SenseVoiceé…ç½® - åŸºäºåç¼–è¯‘APKåˆ†æ
Future<sherpa_onnx.OfflineRecognizer> createSenseVoiceRecognizer() async {
  final modelDir =
      'assets/models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17';

  final senseVoiceConfig = sherpa_onnx.OfflineSenseVoiceModelConfig(
    model: await copyAssetFile('$modelDir/model.int8.onnx'), // ç”¨æˆ·è¦æ±‚ä½¿ç”¨int8æ¨¡å‹
    language: '', // ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹è¯­è¨€
    useInverseTextNormalization: true, // ğŸ¯ å…³é”®ï¼šå¯ç”¨æ ‡ç‚¹ç¬¦å·å¤„ç†
  );

  final modelConfig = sherpa_onnx.OfflineModelConfig(
    senseVoice: senseVoiceConfig,
    tokens: await copyAssetFile('$modelDir/tokens.txt'),
    modelType: 'sense_voice',
    numThreads: 2, // ğŸ¯ åç¼–è¯‘APKä½¿ç”¨2çº¿ç¨‹ï¼Œä¸æ˜¯1çº¿ç¨‹
  );

  final config = sherpa_onnx.OfflineRecognizerConfig(
    model: modelConfig,
    // ğŸ¯ æ·»åŠ åç¼–è¯‘APKä¸­çš„å…³é”®é…ç½®
    decodingMethod: 'greedy_search',
    maxActivePaths: 4,
  );

  return sherpa_onnx.OfflineRecognizer(config);
}

// ğŸ”§ ä¼˜åŒ–VADé…ç½® - åŸºäºåç¼–è¯‘APKåˆ†æ
Future<sherpa_onnx.VoiceActivityDetector> createVAD() async {
  final sileroVadConfig = sherpa_onnx.SileroVadModelConfig(
    model: await copyAssetFile('assets/silero_vad.onnx'),
    minSilenceDuration: 0.5, // ğŸ¯ è°ƒæ•´å‚æ•°æé«˜å®æ—¶æ€§
    minSpeechDuration: 0.25, // ğŸ¯ å‡å°‘æœ€å°è¯­éŸ³æ—¶é•¿
    maxSpeechDuration: 10.0, // ğŸ¯ å¢åŠ æœ€å¤§è¯­éŸ³æ—¶é•¿
    threshold: 0.5, // ğŸ¯ æ·»åŠ é˜ˆå€¼é…ç½®
  );

  final vadConfig = sherpa_onnx.VadModelConfig(
    sileroVad: sileroVadConfig,
    sampleRate: 16000,
    numThreads: 2, // ğŸ¯ VADä¹Ÿä½¿ç”¨2çº¿ç¨‹
  );

  return sherpa_onnx.VoiceActivityDetector(
      config: vadConfig, bufferSizeInSeconds: 30); // ğŸ¯ å¢åŠ ç¼“å†²åŒº
}

class SimulateStreamingAsrScreen extends StatefulWidget {
  const SimulateStreamingAsrScreen({super.key});

  @override
  State<SimulateStreamingAsrScreen> createState() =>
      _SimulateStreamingAsrScreenState();
}

class _SimulateStreamingAsrScreenState
    extends State<SimulateStreamingAsrScreen> {
  late final TextEditingController _controller;
  late final AudioRecorder _audioRecorder;

  String _title = 'SenseVoiceå¤šè¯­è¨€å®æ—¶è¯†åˆ« (å®æ—¶ç‰ˆ)';
  String _last = '';
  int _index = 0;
  bool _isInitialized = false;

  sherpa_onnx.OfflineRecognizer? _recognizer;
  sherpa_onnx.VoiceActivityDetector? _vad;
  int _sampleRate = 16000;

  StreamSubscription<RecordState>? _recordSub;
  RecordState _recordState = RecordState.stop;

  // ğŸ¯ å…³é”®ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§åç¼–è¯‘APKçš„åŒåç¨‹æ¶æ„
  // AnonymousClass1: éŸ³é¢‘å½•åˆ¶åç¨‹ï¼ˆå¯¹åº”samplesChannel.sendï¼‰
  StreamController<Float32List> _samplesChannel =
      StreamController<Float32List>.broadcast();

  // AnonymousClass2: éŸ³é¢‘å¤„ç†åç¨‹çŠ¶æ€å˜é‡ï¼ˆä¸¥æ ¼æŒ‰åç¼–è¯‘APKï¼‰
  List<double> _audioBuffer = [];
  String _lastText = '';
  int _offset = 0;
  int _windowSize = 8000; // 0.5ç§’çª—å£
  bool _isSpeechStarted = false;
  int _startTime = 0;
  bool _added = false;

  // å®æ—¶æ–‡æœ¬æ˜¾ç¤º
  List<String> _resultList = [];
  String _currentPartialText = '';

  // åç¨‹æ§åˆ¶
  StreamSubscription<List<int>>? _audioStreamSub;
  StreamSubscription<Float32List>? _processingStreamSub;

  @override
  void initState() {
    _audioRecorder = AudioRecorder();
    _controller = TextEditingController();

    _recordSub = _audioRecorder.onStateChanged().listen((recordState) {
      _updateRecordState(recordState);
    });

    super.initState();
  }

  Future<void> _start() async {
    if (!_isInitialized) {
      print('ğŸ”„ æ­£åœ¨åˆå§‹åŒ–SenseVoiceå’ŒVAD...');
      sherpa_onnx.initBindings();

      try {
        _recognizer = await createSenseVoiceRecognizer();
        _vad = await createVAD();
        _isInitialized = true;

        print('âœ… SenseVoiceè¯†åˆ«å™¨å’ŒVADåˆå§‹åŒ–æˆåŠŸ');
      } catch (e) {
        print('âŒ åˆå§‹åŒ–å¤±è´¥: $e');
        return;
      }
    }

    try {
      if (await _audioRecorder.hasPermission()) {
        const encoder = AudioEncoder.pcm16bits;

        if (!await _isEncoderSupported(encoder)) {
          return;
        }

        const config = RecordConfig(
          encoder: encoder,
          sampleRate: 16000,
          numChannels: 1,
        );

        // ğŸ¯ é‡ç‚¹ï¼šä¸¥æ ¼æŒ‰ç…§åç¼–è¯‘APKçš„åŒåç¨‹æ¶æ„
        final stream = await _audioRecorder.startStream(config);

        // ğŸ¯ AnonymousClass1: éŸ³é¢‘å½•åˆ¶åç¨‹ (Dispatchers.IO)
        _startAudioRecordingCoroutine(stream);

        // ğŸ¯ AnonymousClass2: éŸ³é¢‘å¤„ç†åç¨‹ (Dispatchers.Default)
        _startAudioProcessingCoroutine();
      }
    } catch (e) {
      print('âŒ å¯åŠ¨å½•éŸ³å¤±è´¥: $e');
    }
  }

  // ğŸ¯ AnonymousClass1: éŸ³é¢‘å½•åˆ¶åç¨‹ï¼ˆä¸¥æ ¼æŒ‰åç¼–è¯‘APKï¼‰
  void _startAudioRecordingCoroutine(Stream<List<int>> audioStream) {
    print('ğŸ™ï¸ å¯åŠ¨éŸ³é¢‘å½•åˆ¶åç¨‹');

    _audioStreamSub = audioStream.listen(
      (rawData) {
        try {
          // ğŸ¯ å…³é”®ï¼šæŒ‰ç…§åç¼–è¯‘APKï¼Œæ¯0.1ç§’çš„éŸ³é¢‘ä½œä¸ºä¸€ä¸ªå•å…ƒ
          final samplesFloat32 =
              convertBytesToFloat32(Uint8List.fromList(rawData));

          // åˆ†å‰²ä¸º0.1ç§’çš„å—ï¼ˆ1600 samples = 16000 * 0.1ï¼‰
          const int chunkSize = 1600;

          for (int i = 0; i < samplesFloat32.length; i += chunkSize) {
            final end = (i + chunkSize < samplesFloat32.length)
                ? i + chunkSize
                : samplesFloat32.length;

            final chunk = Float32List.fromList(samplesFloat32.sublist(i, end));

            // ğŸ¯ å…³é”®ï¼šæ¨¡æ‹ŸsamplesChannel.send(floatSamples)
            if (!_samplesChannel.isClosed) {
              _samplesChannel.add(chunk);
            }
          }
        } catch (e) {
          print('éŸ³é¢‘å½•åˆ¶åç¨‹é”™è¯¯: $e');
        }
      },
      onDone: () {
        print('ğŸ”‡ éŸ³é¢‘å½•åˆ¶åç¨‹åœæ­¢');
      },
    );
  }

  // ğŸ¯ AnonymousClass2: éŸ³é¢‘å¤„ç†åç¨‹ï¼ˆä¸¥æ ¼æŒ‰åç¼–è¯‘APKçš„å¤æ‚é€»è¾‘ï¼‰
  void _startAudioProcessingCoroutine() {
    print('ğŸ”„ å¯åŠ¨éŸ³é¢‘å¤„ç†åç¨‹');

    // é‡ç½®çŠ¶æ€å˜é‡ï¼ˆæŒ‰åç¼–è¯‘APKï¼‰
    _audioBuffer.clear();
    _lastText = '';
    _offset = 0;
    _windowSize = 8000; // 0.5ç§’çª—å£
    _isSpeechStarted = false;
    _startTime = DateTime.now().millisecondsSinceEpoch;
    _added = false;
    _vad?.reset();

    _processingStreamSub = _samplesChannel.stream.listen(
      (audioChunk) {
        _processAudioChunkRealtime(audioChunk);
      },
      onDone: () {
        print('ğŸ”‡ éŸ³é¢‘å¤„ç†åç¨‹åœæ­¢');
      },
    );
  }

  // ğŸ¯ æ ¸å¿ƒï¼šå®æ—¶éŸ³é¢‘å¤„ç†ï¼ˆå¤åˆ»åç¼–è¯‘APKçš„å¤æ‚é€»è¾‘ï¼‰
  void _processAudioChunkRealtime(Float32List audioChunk) {
    try {
      // æ·»åŠ åˆ°æ»‘åŠ¨ç¼“å†²åŒº
      _audioBuffer.addAll(audioChunk);

      // ğŸ¯ å…³é”®ï¼šæ»‘åŠ¨çª—å£æœºåˆ¶ï¼ˆæ¨¡æ‹Ÿåç¼–è¯‘APKçš„buffer+offseté€»è¾‘ï¼‰
      if (_audioBuffer.length > _windowSize * 2) {
        // ä¿æŒçª—å£å¤§å°ï¼Œç§»é™¤æ—§æ•°æ®
        _audioBuffer = _audioBuffer.sublist(_audioBuffer.length - _windowSize);
        _offset = _audioBuffer.length - _windowSize;
      }

      // å®æ—¶VADå¤„ç†
      if (_audioBuffer.length >= _windowSize ~/ 4) {
        // è¶³å¤Ÿçš„æ•°æ®è¿›è¡ŒVAD
        _processRealtimeVAD();
      }

      // ğŸ¯ å…³é”®ï¼šå®æ—¶è¯†åˆ«ï¼ˆä¸ç­‰å¾…VADå®Œæˆï¼‰
      if (_audioBuffer.length >= _windowSize ~/ 2) {
        // 0.25ç§’æ•°æ®
        _processRealtimeRecognition();
      }
    } catch (e) {
      print('å®æ—¶éŸ³é¢‘å¤„ç†é”™è¯¯: $e');
    }
  }

  // ğŸ¯ å®æ—¶VADå¤„ç†
  void _processRealtimeVAD() {
    try {
      if (_vad == null || _audioBuffer.length < 1600) return;

      // å–æœ€è¿‘çš„éŸ³é¢‘è¿›è¡ŒVAD
      final recentAudio = Float32List.fromList(
          _audioBuffer.sublist(math.max(0, _audioBuffer.length - 1600)));

      _vad!.acceptWaveform(recentAudio);

      // å¤„ç†VADæ£€æµ‹åˆ°çš„è¯­éŸ³æ®µ
      while (!_vad!.isEmpty()) {
        final speechSegment = _vad!.front();
        _vad!.pop();

        // å®Œæ•´è¯­éŸ³æ®µè¯†åˆ«
        _processFinalSpeechSegment(speechSegment);
      }
    } catch (e) {
      print('VADå¤„ç†é”™è¯¯: $e');
    }
  }

  // ğŸ¯ å®æ—¶è¯†åˆ«ï¼ˆè¾¹è¯´è¾¹æ˜¾ç¤ºçš„å…³é”®ï¼‰
  void _processRealtimeRecognition() {
    try {
      if (_recognizer == null || _audioBuffer.length < 3200) return; // 0.2ç§’æ•°æ®

      // ğŸ¯ å…³é”®ï¼šæ»‘åŠ¨çª—å£å®æ—¶è¯†åˆ«
      final windowAudio = Float32List.fromList(
          _audioBuffer.sublist(math.max(0, _audioBuffer.length - _windowSize)));

      final stream = _recognizer!.createStream();
      stream.acceptWaveform(samples: windowAudio, sampleRate: _sampleRate);

      _recognizer!.decode(stream);
      final result = _recognizer!.getResult(stream);
      final text = result.text.trim();

      // ğŸ¯ å®æ—¶æ–‡æœ¬æ›´æ–°ï¼ˆè¾¹è¯´è¾¹æ˜¾ç¤ºï¼‰
      if (text.isNotEmpty && text != _lastText) {
        _updatePartialText(text);
        _lastText = text;
      }

      stream.free();
    } catch (e) {
      print('å®æ—¶è¯†åˆ«é”™è¯¯: $e');
    }
  }

  // ğŸ¯ å®Œæ•´è¯­éŸ³æ®µå¤„ç†ï¼ˆæœ€ç»ˆç»“æœï¼‰
  void _processFinalSpeechSegment(dynamic speechSegment) {
    try {
      final stream = _recognizer!.createStream();
      stream.acceptWaveform(
          samples: speechSegment.samples, sampleRate: _sampleRate);

      _recognizer!.decode(stream);
      final result = _recognizer!.getResult(stream);
      final text = result.text.trim();

      if (text.isNotEmpty) {
        _addFinalResult(text);
      }

      stream.free();
    } catch (e) {
      print('å®Œæ•´è¯­éŸ³æ®µå¤„ç†é”™è¯¯: $e');
    }
  }

  // ğŸ¯ å®æ—¶æ–‡æœ¬æ›´æ–°ï¼ˆéƒ¨åˆ†ç»“æœï¼Œè¾¹è¯´è¾¹æ˜¾ç¤ºï¼‰
  void _updatePartialText(String text) {
    setState(() {
      _currentPartialText = text;

      // æ˜¾ç¤ºå½“å‰çš„éƒ¨åˆ†ç»“æœå’Œå†å²ç»“æœ
      String displayText = '';
      for (int i = 0; i < _resultList.length; i++) {
        displayText += '$i: ${_resultList[i]}\n';
      }

      // æ·»åŠ å½“å‰æ­£åœ¨è¯†åˆ«çš„æ–‡æœ¬
      if (_currentPartialText.isNotEmpty) {
        displayText += '${_resultList.length}: $_currentPartialText (è¯†åˆ«ä¸­...)';
      }

      _controller.value = TextEditingValue(
        text: displayText,
        selection: TextSelection.collapsed(offset: displayText.length),
      );
    });
  }

  // ğŸ¯ æ·»åŠ æœ€ç»ˆç»“æœ
  void _addFinalResult(String text) {
    setState(() {
      _resultList.add(text);
      _currentPartialText = ''; // æ¸…ç©ºéƒ¨åˆ†ç»“æœ

      // æ›´æ–°æ˜¾ç¤º
      String displayText = '';
      for (int i = 0; i < _resultList.length; i++) {
        displayText += '$i: ${_resultList[i]}';
        if (i < _resultList.length - 1) displayText += '\n';
      }

      _controller.value = TextEditingValue(
        text: displayText,
        selection: TextSelection.collapsed(offset: displayText.length),
      );
    });

    print('âœ… æœ€ç»ˆç»“æœ: $text');
  }

  Future<void> _stop() async {
    _audioStreamSub?.cancel();
    _processingStreamSub?.cancel();
    _samplesChannel.close();

    // é‡æ–°åˆ›å»ºchannelä¸ºä¸‹æ¬¡ä½¿ç”¨
    _samplesChannel = StreamController<Float32List>.broadcast();

    _audioBuffer.clear();
    _currentPartialText = '';
    await _audioRecorder.stop();
  }

  Future<void> _pause() => _audioRecorder.pause();
  Future<void> _resume() => _audioRecorder.resume();

  void _updateRecordState(RecordState recordState) {
    setState(() => _recordState = recordState);
  }

  Future<bool> _isEncoderSupported(AudioEncoder encoder) async {
    final isSupported = await _audioRecorder.isEncoderSupported(encoder);

    if (!isSupported) {
      debugPrint('${encoder.name} is not supported on this platform.');
      debugPrint('Supported encoders are:');

      for (final e in AudioEncoder.values) {
        if (await _audioRecorder.isEncoderSupported(e)) {
          debugPrint('- ${e.name}');
        }
      }
    }

    return isSupported;
  }

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: Scaffold(
        appBar: AppBar(
          title: Text(_title),
          backgroundColor: Colors.blue[700], // ğŸ¯ è“è‰²è¡¨ç¤ºå®æ—¶ç‰ˆ
          foregroundColor: Colors.white,
        ),
        body: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: [
            Container(
              padding: const EdgeInsets.all(16),
              margin: const EdgeInsets.all(16),
              decoration: BoxDecoration(
                color: Colors.blue[50], // ğŸ¯ é…è‰²æ›´æ–°
                borderRadius: BorderRadius.circular(8),
                border: Border.all(color: Colors.blue[200]!),
              ),
              child: Column(
                children: [
                  const Icon(Icons.mic_external_on,
                      color: Colors.blue), // ğŸ¯ å®æ—¶å›¾æ ‡
                  const SizedBox(height: 8),
                  const Text(
                    'SenseVoiceå¤šè¯­è¨€æ¨¡å‹ (å®æ—¶ç‰ˆ)',
                    style: TextStyle(fontWeight: FontWeight.bold),
                  ),
                  const SizedBox(height: 4),
                  const Text(
                    'ğŸ™ï¸ åŒåç¨‹æ¶æ„ | âš¡ è¾¹è¯´è¾¹æ˜¾ç¤º | ğŸ¯ æ»‘åŠ¨çª—å£',
                    style: TextStyle(fontSize: 12, color: Colors.grey),
                  ),
                  const SizedBox(height: 4),
                  Text(
                    _isInitialized ? 'âœ… å·²åˆå§‹åŒ–' : 'â³ åˆå§‹åŒ–ä¸­...',
                    style: TextStyle(
                      fontSize: 12,
                      color: _isInitialized ? Colors.green : Colors.orange,
                    ),
                  ),
                ],
              ),
            ),
            Expanded(
              child: Container(
                margin: const EdgeInsets.all(16),
                padding: const EdgeInsets.all(16),
                decoration: BoxDecoration(
                  border: Border.all(color: Colors.grey[300]!),
                  borderRadius: BorderRadius.circular(8),
                ),
                child: TextField(
                  maxLines: null,
                  expands: true,
                  controller: _controller,
                  readOnly: true,
                  style: const TextStyle(fontSize: 16),
                  decoration: const InputDecoration(
                    border: InputBorder.none,
                    hintText: 'å¼€å§‹è¯´è¯ï¼Œå®æ—¶è¯†åˆ«ç»“æœå°†è¾¹è¯´è¾¹æ˜¾ç¤º...',
                  ),
                ),
              ),
            ),
            const SizedBox(height: 20),
            Row(
              mainAxisAlignment: MainAxisAlignment.center,
              children: <Widget>[
                _buildRecordStopControl(),
                const SizedBox(width: 20),
                _buildText(),
              ],
            ),
            const SizedBox(height: 50),
          ],
        ),
      ),
    );
  }

  @override
  void dispose() {
    _audioStreamSub?.cancel();
    _processingStreamSub?.cancel();
    _samplesChannel.close();
    _recordSub?.cancel();
    _audioRecorder.dispose();
    _recognizer?.free();
    _vad?.free();
    super.dispose();
  }

  Widget _buildRecordStopControl() {
    late Icon icon;
    late Color color;

    if (_recordState != RecordState.stop) {
      icon = const Icon(Icons.stop, color: Colors.red, size: 30);
      color = Colors.red.withOpacity(0.1);
    } else {
      final theme = Theme.of(context);
      icon = Icon(Icons.mic, color: theme.primaryColor, size: 30);
      color = theme.primaryColor.withOpacity(0.1);
    }

    return ClipOval(
      child: Material(
        color: color,
        child: InkWell(
          child: SizedBox(width: 56, height: 56, child: icon),
          onTap: () {
            (_recordState != RecordState.stop) ? _stop() : _start();
          },
        ),
      ),
    );
  }

  Widget _buildText() {
    if (_recordState == RecordState.stop) {
      return const Text("å¼€å§‹å½•éŸ³");
    } else {
      return const Text("åœæ­¢å½•éŸ³");
    }
  }
}
