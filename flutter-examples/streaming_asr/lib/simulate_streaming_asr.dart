// Copyright (c)  2024  Xiaomi Corporation
import 'dart:async';
import 'dart:typed_data';
import 'dart:collection';

import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';
import 'package:path/path.dart' as p;
import 'package:path_provider/path_provider.dart';
import 'package:record/record.dart';

import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
import './utils.dart';

// ğŸ”§ ä¼˜åŒ–åçš„SenseVoiceé…ç½® - åŸºäºåç¼–è¯‘APKåˆ†æ
Future<sherpa_onnx.OfflineRecognizer> createSenseVoiceRecognizer() async {
  final modelDir =
      'assets/models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17';

  final senseVoiceConfig = sherpa_onnx.OfflineSenseVoiceModelConfig(
    model: await copyAssetFile('$modelDir/model.int8.onnx'), // ç”¨æˆ·è¦æ±‚ä½¿ç”¨int8æ¨¡å‹
    language: '', // ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹è¯­è¨€
    useInverseTextNormalization: true, // ğŸ¯ å…³é”®ï¼šå¯ç”¨æ ‡ç‚¹ç¬¦å·å¤„ç†
  );

  final modelConfig = sherpa_onnx.OfflineModelConfig(
    senseVoice: senseVoiceConfig,
    tokens: await copyAssetFile('$modelDir/tokens.txt'),
    modelType: 'sense_voice',
    numThreads: 2, // ğŸ¯ åç¼–è¯‘APKä½¿ç”¨2çº¿ç¨‹ï¼Œä¸æ˜¯1çº¿ç¨‹
  );

  final config = sherpa_onnx.OfflineRecognizerConfig(
    model: modelConfig,
    // ğŸ¯ æ·»åŠ åç¼–è¯‘APKä¸­çš„å…³é”®é…ç½®
    decodingMethod: 'greedy_search',
    maxActivePaths: 4,
  );

  return sherpa_onnx.OfflineRecognizer(config);
}

// ğŸ”§ ä¼˜åŒ–VADé…ç½® - åŸºäºåç¼–è¯‘APKåˆ†æ
Future<sherpa_onnx.VoiceActivityDetector> createVAD() async {
  final sileroVadConfig = sherpa_onnx.SileroVadModelConfig(
    model: await copyAssetFile('assets/silero_vad.onnx'),
    minSilenceDuration: 0.5, // ğŸ¯ è°ƒæ•´å‚æ•°æé«˜å®æ—¶æ€§
    minSpeechDuration: 0.25, // ğŸ¯ å‡å°‘æœ€å°è¯­éŸ³æ—¶é•¿
    maxSpeechDuration: 10.0, // ğŸ¯ å¢åŠ æœ€å¤§è¯­éŸ³æ—¶é•¿
    threshold: 0.5, // ğŸ¯ æ·»åŠ é˜ˆå€¼é…ç½®
  );

  final vadConfig = sherpa_onnx.VadModelConfig(
    sileroVad: sileroVadConfig,
    sampleRate: 16000,
    numThreads: 2, // ğŸ¯ VADä¹Ÿä½¿ç”¨2çº¿ç¨‹
  );

  return sherpa_onnx.VoiceActivityDetector(
      config: vadConfig, bufferSizeInSeconds: 30); // ğŸ¯ å¢åŠ ç¼“å†²åŒº
}

class SimulateStreamingAsrScreen extends StatefulWidget {
  const SimulateStreamingAsrScreen({super.key});

  @override
  State<SimulateStreamingAsrScreen> createState() =>
      _SimulateStreamingAsrScreenState();
}

class _SimulateStreamingAsrScreenState
    extends State<SimulateStreamingAsrScreen> {
  late final TextEditingController _controller;
  late final AudioRecorder _audioRecorder;

  String _title = 'SenseVoiceå¤šè¯­è¨€å®æ—¶è¯†åˆ« (ä¿®å¤ç‰ˆ)';
  String _last = '';
  int _index = 0;
  bool _isInitialized = false;

  sherpa_onnx.OfflineRecognizer? _recognizer;
  sherpa_onnx.VoiceActivityDetector? _vad;
  int _sampleRate = 16000;

  StreamSubscription<RecordState>? _recordSub;
  RecordState _recordState = RecordState.stop;

  // ğŸ¯ æ ¸å¿ƒä¿®å¤ï¼šé‡‡ç”¨åç¼–è¯‘APKçš„Channelé˜Ÿåˆ—æœºåˆ¶
  final Queue<Float32List> _audioQueue = Queue<Float32List>();
  StreamSubscription<List<int>>? _audioStreamSub;
  Timer? _processingTimer;

  // ğŸ¯ é¿å…é‡å¤å¤„ç†çš„çŠ¶æ€ç®¡ç†
  bool _isProcessing = false;

  @override
  void initState() {
    _audioRecorder = AudioRecorder();
    _controller = TextEditingController();

    _recordSub = _audioRecorder.onStateChanged().listen((recordState) {
      _updateRecordState(recordState);
    });

    super.initState();
  }

  Future<void> _start() async {
    if (!_isInitialized) {
      print('ğŸ”„ æ­£åœ¨åˆå§‹åŒ–SenseVoiceå’ŒVAD...');
      sherpa_onnx.initBindings();

      try {
        _recognizer = await createSenseVoiceRecognizer();
        _vad = await createVAD();
        _isInitialized = true;

        print('âœ… SenseVoiceè¯†åˆ«å™¨å’ŒVADåˆå§‹åŒ–æˆåŠŸ');
      } catch (e) {
        print('âŒ åˆå§‹åŒ–å¤±è´¥: $e');
        return;
      }
    }

    try {
      if (await _audioRecorder.hasPermission()) {
        const encoder = AudioEncoder.pcm16bits;

        if (!await _isEncoderSupported(encoder)) {
          return;
        }

        const config = RecordConfig(
          encoder: encoder,
          sampleRate: 16000,
          numChannels: 1,
        );

        // ğŸ¯ å…³é”®ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§åç¼–è¯‘APKçš„æ¶æ„
        final stream = await _audioRecorder.startStream(config);

        // ğŸ¯ éŸ³é¢‘å½•åˆ¶åç¨‹ï¼šæ¨¡æ‹Ÿåç¼–è¯‘APKçš„samplesChannel.sendé€»è¾‘
        _audioStreamSub = stream.listen(
          (rawData) {
            _enqueueAudioData(rawData);
          },
          onDone: () {
            print('ğŸ”‡ éŸ³é¢‘æµåœæ­¢');
          },
        );

        // ğŸ¯ éŸ³é¢‘å¤„ç†åç¨‹ï¼šä»é˜Ÿåˆ—å–æ•°æ®å¤„ç†ï¼Œé¿å…é‡å¤
        _processingTimer = Timer.periodic(
          const Duration(milliseconds: 100), // ğŸ”§ 100mså¤„ç†é—´éš”
          (_) => _processAudioQueue(),
        );
      }
    } catch (e) {
      print('âŒ å¯åŠ¨å½•éŸ³å¤±è´¥: $e');
    }
  }

  // ğŸ¯ æ–°å¢ï¼šéŸ³é¢‘å…¥é˜Ÿé€»è¾‘ (å¯¹åº”åç¼–è¯‘APKçš„samplesChannel.send)
  void _enqueueAudioData(List<int> rawData) {
    try {
      // è½¬æ¢ä¸ºFloat32List
      final samplesFloat32 = convertBytesToFloat32(Uint8List.fromList(rawData));

      // ğŸ¯ å…³é”®ï¼šæŒ‰ç…§åç¼–è¯‘APKï¼Œæ¯0.1ç§’çš„æ•°æ®ä½œä¸ºä¸€ä¸ªå•å…ƒ
      const int chunkSize = 1600; // 16000 * 0.1 = 1600 samples

      for (int i = 0; i < samplesFloat32.length; i += chunkSize) {
        final end = (i + chunkSize < samplesFloat32.length)
            ? i + chunkSize
            : samplesFloat32.length;

        final chunk = Float32List.fromList(samplesFloat32.sublist(i, end));

        // å…¥é˜Ÿï¼Œé™åˆ¶é˜Ÿåˆ—å¤§å°é¿å…å†…å­˜æ³„æ¼
        _audioQueue.add(chunk);

        // ä¿æŒé˜Ÿåˆ—å¤§å°åˆç† (æœ€å¤š30ç§’æ•°æ®)
        while (_audioQueue.length > 300) {
          // 30ç§’ / 0.1ç§’ = 300å—
          _audioQueue.removeFirst();
        }
      }
    } catch (e) {
      print('éŸ³é¢‘å…¥é˜Ÿå¤±è´¥: $e');
    }
  }

  // ğŸ¯ æ–°å¢ï¼šéŸ³é¢‘é˜Ÿåˆ—å¤„ç†é€»è¾‘ (å¯¹åº”åç¼–è¯‘APKçš„Channel.receiveé€»è¾‘)
  void _processAudioQueue() {
    if (_isProcessing ||
        _audioQueue.isEmpty ||
        _vad == null ||
        _recognizer == null) {
      return;
    }

    _isProcessing = true;

    try {
      // ğŸ¯ å…³é”®ï¼šæ¯æ¬¡åªå¤„ç†é˜Ÿåˆ—ä¸­çš„æ–°æ•°æ®ï¼Œé¿å…é‡å¤
      final List<Float32List> currentBatch = [];

      // å–å‡ºæ‰€æœ‰å¾…å¤„ç†çš„éŸ³é¢‘å—
      while (_audioQueue.isNotEmpty && currentBatch.length < 10) {
        // æœ€å¤šå¤„ç†1ç§’æ•°æ®
        currentBatch.add(_audioQueue.removeFirst());
      }

      if (currentBatch.isEmpty) {
        _isProcessing = false;
        return;
      }

      // ğŸ¯ åˆå¹¶éŸ³é¢‘å—è¿›è¡ŒVADå¤„ç†
      final List<double> combinedAudio = [];
      for (final chunk in currentBatch) {
        combinedAudio.addAll(chunk);
      }

      final Float32List samples = Float32List.fromList(combinedAudio);

      // VADå¤„ç†
      _vad!.acceptWaveform(samples);

      // ğŸ¯ å…³é”®ï¼šåªæœ‰å½“VADæ£€æµ‹åˆ°å®Œæ•´è¯­éŸ³æ®µæ—¶æ‰è¿›è¡Œè¯†åˆ«
      while (!_vad!.isEmpty()) {
        final speechSegment = _vad!.front();
        _vad!.pop();

        _processCompleteSpeechSegment(speechSegment);
      }
    } catch (e) {
      print('âŒ é˜Ÿåˆ—å¤„ç†å¤±è´¥: $e');
    } finally {
      _isProcessing = false;
    }
  }

  // ğŸ¯ å¤„ç†å®Œæ•´è¯­éŸ³æ®µ (å•æ¬¡è¯†åˆ«ï¼Œæ— é‡å¤)
  void _processCompleteSpeechSegment(dynamic speechSegment) {
    try {
      final stream = _recognizer!.createStream();
      stream.acceptWaveform(
          samples: speechSegment.samples, sampleRate: _sampleRate);

      _recognizer!.decode(stream);
      final result = _recognizer!.getResult(stream);
      final text = result.text.trim();

      if (text.isNotEmpty) {
        _updateFinalText(text);
      }

      stream.free();
    } catch (e) {
      print('è¯­éŸ³æ®µè¯†åˆ«é”™è¯¯: $e');
    }
  }

  // ğŸ¯ æœ€ç»ˆæ–‡æœ¬æ›´æ–° (ç¡®ä¿ä¸é‡å¤)
  void _updateFinalText(String text) {
    setState(() {
      String textToDisplay;
      if (_last.isEmpty) {
        textToDisplay = '$_index: $text';
      } else {
        textToDisplay = '$_index: $text\n$_last';
      }

      _last = textToDisplay;
      _index += 1;

      _controller.value = TextEditingValue(
        text: textToDisplay,
        selection: TextSelection.collapsed(offset: textToDisplay.length),
      );
    });

    print('âœ… è¯†åˆ«ç»“æœ: $text'); // è°ƒè¯•æ—¥å¿—
  }

  Future<void> _stop() async {
    _audioStreamSub?.cancel();
    _processingTimer?.cancel();
    _audioQueue.clear();
    _isProcessing = false;
    await _audioRecorder.stop();
  }

  Future<void> _pause() => _audioRecorder.pause();
  Future<void> _resume() => _audioRecorder.resume();

  void _updateRecordState(RecordState recordState) {
    setState(() => _recordState = recordState);
  }

  Future<bool> _isEncoderSupported(AudioEncoder encoder) async {
    final isSupported = await _audioRecorder.isEncoderSupported(encoder);

    if (!isSupported) {
      debugPrint('${encoder.name} is not supported on this platform.');
      debugPrint('Supported encoders are:');

      for (final e in AudioEncoder.values) {
        if (await _audioRecorder.isEncoderSupported(e)) {
          debugPrint('- ${e.name}');
        }
      }
    }

    return isSupported;
  }

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: Scaffold(
        appBar: AppBar(
          title: Text(_title),
          backgroundColor: Colors.orange[700], // ğŸ¯ æ©™è‰²è¡¨ç¤ºä¿®å¤ç‰ˆ
          foregroundColor: Colors.white,
        ),
        body: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: [
            Container(
              padding: const EdgeInsets.all(16),
              margin: const EdgeInsets.all(16),
              decoration: BoxDecoration(
                color: Colors.orange[50], // ğŸ¯ é…è‰²æ›´æ–°
                borderRadius: BorderRadius.circular(8),
                border: Border.all(color: Colors.orange[200]!),
              ),
              child: Column(
                children: [
                  const Icon(Icons.build_circle,
                      color: Colors.orange), // ğŸ¯ ä¿®å¤å›¾æ ‡
                  const SizedBox(height: 8),
                  const Text(
                    'SenseVoiceå¤šè¯­è¨€æ¨¡å‹ (ä¿®å¤ç‰ˆ)',
                    style: TextStyle(fontWeight: FontWeight.bold),
                  ),
                  const SizedBox(height: 4),
                  const Text(
                    'ğŸ”§ ä¿®å¤é‡å¤è¯†åˆ« | âœ¨ æ ‡ç‚¹ç¬¦å· | ğŸ“‹ é˜Ÿåˆ—æœºåˆ¶',
                    style: TextStyle(fontSize: 12, color: Colors.grey),
                  ),
                  const SizedBox(height: 4),
                  Text(
                    _isInitialized ? 'âœ… å·²åˆå§‹åŒ–' : 'â³ åˆå§‹åŒ–ä¸­...',
                    style: TextStyle(
                      fontSize: 12,
                      color: _isInitialized ? Colors.green : Colors.orange,
                    ),
                  ),
                ],
              ),
            ),
            Expanded(
              child: Container(
                margin: const EdgeInsets.all(16),
                padding: const EdgeInsets.all(16),
                decoration: BoxDecoration(
                  border: Border.all(color: Colors.grey[300]!),
                  borderRadius: BorderRadius.circular(8),
                ),
                child: TextField(
                  maxLines: null,
                  expands: true,
                  controller: _controller,
                  readOnly: true,
                  style: const TextStyle(fontSize: 16),
                  decoration: const InputDecoration(
                    border: InputBorder.none,
                    hintText: 'è¯´è¯å®Œæ•´å¥å­åä¼šæ˜¾ç¤ºè¯†åˆ«ç»“æœ...',
                  ),
                ),
              ),
            ),
            const SizedBox(height: 20),
            Row(
              mainAxisAlignment: MainAxisAlignment.center,
              children: <Widget>[
                _buildRecordStopControl(),
                const SizedBox(width: 20),
                _buildText(),
              ],
            ),
            const SizedBox(height: 50),
          ],
        ),
      ),
    );
  }

  @override
  void dispose() {
    _audioStreamSub?.cancel();
    _processingTimer?.cancel();
    _recordSub?.cancel();
    _audioRecorder.dispose();
    _recognizer?.free();
    _vad?.free();
    super.dispose();
  }

  Widget _buildRecordStopControl() {
    late Icon icon;
    late Color color;

    if (_recordState != RecordState.stop) {
      icon = const Icon(Icons.stop, color: Colors.red, size: 30);
      color = Colors.red.withOpacity(0.1);
    } else {
      final theme = Theme.of(context);
      icon = Icon(Icons.mic, color: theme.primaryColor, size: 30);
      color = theme.primaryColor.withOpacity(0.1);
    }

    return ClipOval(
      child: Material(
        color: color,
        child: InkWell(
          child: SizedBox(width: 56, height: 56, child: icon),
          onTap: () {
            (_recordState != RecordState.stop) ? _stop() : _start();
          },
        ),
      ),
    );
  }

  Widget _buildText() {
    if (_recordState == RecordState.stop) {
      return const Text("å¼€å§‹å½•éŸ³");
    } else {
      return const Text("åœæ­¢å½•éŸ³");
    }
  }
}
